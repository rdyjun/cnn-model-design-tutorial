{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 모듈 포함하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터 전처리\n",
    "데이터 모델이 데이터를 처리하기 쉬운 형태로 변환하기 위한 목적<br>\n",
    "데이터셋 로드 전 데이터셋 전처리 방식 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 데이터셋 준비\n",
    "학습시키고자 하는 데이터셋 로드 및 전처리 적용<br>\n",
    "학습이 목적인 `trainset`과 모델 정확도 테스트를 위한 `testset`으로 분류<br>\n",
    "또한 CNN모델에 입력할 수 있도록 각 `trainset`과 `testset`을 `DataLoader`를 사용하여 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=4, shuffle=True)\n",
    "\n",
    "testset = datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=4, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN 모델 함수로 구현\n",
    "Convolution 연산, Max Pool 기술을 사용하여 특징점을 추출하고,<br>\n",
    "추출한 특징점을 가지고 이미지를 분류하는 기술"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__() \n",
    "        self.conv1 = nn.Conv2d(3, 6, 5)\n",
    "        self.pool = nn.MaxPool2d(2, 2) \n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, x):\n",
    "        x = self.pool(F.relu(self.conv1(x)))\n",
    "        x = self.pool(F.relu(self.conv2(x)))\n",
    "        x = x.view(-1, 16 * 5 * 5)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x\n",
    "\n",
    "model = CNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습\n",
    "데이터셋의 학습 데이터를 통해 모델을 학습시키는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f'Train Epoch: {epoch+1} [{batch_idx * len(data)}/{len(train_loader.dataset)} ({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 테스트\n",
    "학습된 모델의 성능을 평가하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, test_loader, epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            data, target = data\n",
    "            output = model(data)\n",
    "            test_loss += criterion(output, target).item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "        f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.0f}%)\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 평가방법\n",
    "1. model에 데이터를 입력해 `예측값` 얻기\n",
    "2. `예측값`과 `실제 레이블` 간 `손실 계산`\n",
    "> 손실이 작을수록 모델의 성능이 좋음\n",
    "3. 예측값과 실제 레이블 `일치 비율` 계산"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 성능 개선 방안\n",
    "1. 데이터 증강\n",
    "> 원본 데이터를 회전, 뒤집기, 확대/축소, 밝기/색상 변경 과정을 통해 데이터 양의 증가\n",
    "2. 배치 정규화\n",
    "> 각 층의 입력 데이터 평균을 0, 분산이 1이 되도록 정규화\n",
    "> > 내부 공변량 변화(데이터 특성 변형)를 해결하기 위한 방법<br>\n",
    "> > 각 층의 출력 시 데이터를 정규화하여 각 층의 역할에 집중시켜 학습속도 향상\n",
    "3. 학습률 스케줄링\n",
    "> 초기에 큰 학습률을 사용하여 빠르게 학습 후 점차 학습률을 감소시켜 최적화<br>\n",
    "> 즉, 초기에 모델을 단순하고 빠르게 학습하고, 점차 모델을 깊이있게 학습<br>\n",
    "> > 경사하강법에서 가중치를 업데이트하는 데 있어서 학습률이 높을 경우 정확한 답을 찾기 어렵고,<br>\n",
    "> > 학습률이 낮을 경우 지역 최소값에 갇히거나, 학습 속도가 매우 낮아짐\n",
    "4. 더 깊은 모델 사용\n",
    "> CNN 내부 컨볼루션, 풀링 레이어를 추가하여 모델 구조를 복잡하게 수정<br>\n",
    "> 추가된 레이어를 통해 복잡한 패턴을 학습할 수 있으나, 과적합 문제가 발생할 수 있음\n",
    "> > `드롭아웃`과 같은 정규화 기법을 통해 과적합을 방지할 수 있음\n",
    "5. Early Stopping\n",
    "> 검증 오차가 더 이상 감소하지 않을 때 학습을 중단<br>\n",
    "> 해당 시점의 모델을 선택하여 학습 시간 단축 효과"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 저장\n",
    "학습된 모델을 저장하는 함수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(model, epoch):\n",
    "    model_path = os.path.join(model_dir, 'mnist_mlp_model_epoch_{}.pth'.format(epoch))\n",
    "    torch.save(model.state_dict(), model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습을 위한 준비 \n",
    "1. 예측값과 실제값 사이 손실을 계산하는 함수 준비<br>\n",
    "2. 모델 학습 과정에서 손실 함수 최소화를 위한 최적화 알고리즘 준비<br>\n",
    "3. 훈련 횟수 준비"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "epochs = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 모델 학습 적용\n",
    "훈련 횟수만큼 `훈련 > 테스트 > 모델 저장` 과정 반복<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(epochs):\n",
    "    train(model, train_loader, optimizer, epoch)\n",
    "    test(model, test_loader, epoch)\n",
    "    save_model(model, epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
